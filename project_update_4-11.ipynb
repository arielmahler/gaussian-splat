{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of the sift algorithm, with the trilinear interpolation smoothing of a histogram tensor of the generate descriptors being implemented in hardware using our overlay, histogram_tensor\n",
    "\n",
    "This overlay stream in inputs for the histogram tensor calculations, which are done on the board, and the resulting structure is stream back out and placed back into the software structure for further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pynq import Overlay\n",
    "overlay = Overlay('/home/xilinx/pynq/overlays/descriptor/design_1.bit')\n",
    "dma = overlay.axi_dma_0\n",
    "dma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfm_algo_unpacked import localizeExtremumViaQuadraticFit, computeKeypointsWithOrientations, isPixelAnExtremum\n",
    "from numpy import floor\n",
    "\n",
    "def findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width, contrast_threshold=0.04):\n",
    "    threshold = floor(0.5 * contrast_threshold / num_intervals * 255)\n",
    "    keypoints = []\n",
    "\n",
    "    for octave_index, dog_images_in_octave in enumerate(dog_images):\n",
    "        for image_index, (first_image, second_image, third_image) in enumerate(zip(dog_images_in_octave, dog_images_in_octave[1:], dog_images_in_octave[2:])):\n",
    "            for i in range(image_border_width, first_image.shape[0] - image_border_width):\n",
    "                if (i % 10 == 0):\n",
    "                    print('.', end='')\n",
    "                if (i % 100 == 0):\n",
    "                    print('')\n",
    "                for j in range(image_border_width, first_image.shape[1] - image_border_width):\n",
    "                    if isPixelAnExtremum(first_image[i-1:i+2, j-1:j+2], second_image[i-1:i+2, j-1:j+2], third_image[i-1:i+2, j-1:j+2], threshold):\n",
    "                        localization_result = localizeExtremumViaQuadraticFit(i, j, image_index + 1, octave_index, num_intervals, dog_images_in_octave, sigma, contrast_threshold, image_border_width)\n",
    "                        if localization_result is not None:\n",
    "                            keypoint, localized_image_index = localization_result\n",
    "                            keypoints_with_orientations = computeKeypointsWithOrientations(keypoint, octave_index, gaussian_images[octave_index][localized_image_index])\n",
    "                            for keypoint_with_orientation in keypoints_with_orientations:\n",
    "                                keypoints.append(keypoint_with_orientation)\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished base_image\n",
      "Finished num_octaves\n",
      "Finished gaussian_kernels\n",
      "Finished gaussian_images\n",
      "Finished dog_images\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m dog_images \u001b[38;5;241m=\u001b[39m generateDoGImages(gaussian_images)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished dog_images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m \u001b[43mfindScaleSpaceExtrema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgaussian_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdog_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_intervals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_border_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished keypoints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m removeDuplicateKeypoints(keypoints)\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mfindScaleSpaceExtrema\u001b[0;34m(gaussian_images, dog_images, num_intervals, sigma, image_border_width, contrast_threshold)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m localization_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     keypoint, localized_image_index \u001b[38;5;241m=\u001b[39m localization_result\n\u001b[0;32m---> 18\u001b[0m     keypoints_with_orientations \u001b[38;5;241m=\u001b[39m \u001b[43mcomputeKeypointsWithOrientations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctave_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaussian_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43moctave_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlocalized_image_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m keypoint_with_orientation \u001b[38;5;129;01min\u001b[39;00m keypoints_with_orientations:\n\u001b[1;32m     20\u001b[0m         keypoints\u001b[38;5;241m.\u001b[39mappend(keypoint_with_orientation)\n",
      "File \u001b[0;32m/home/xilinx/jupyter_notebooks/Project/temp/sfm_algo_unpacked.py:187\u001b[0m, in \u001b[0;36mcomputeKeypointsWithOrientations\u001b[0;34m(keypoint, octave_index, gaussian_image, radius_factor, num_bins, peak_ratio, scale_factor)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 gradient_orientation \u001b[38;5;241m=\u001b[39m rad2deg(arctan2(dy, dx))\n\u001b[1;32m    186\u001b[0m                 weight \u001b[38;5;241m=\u001b[39m exp(weight_factor \u001b[38;5;241m*\u001b[39m (i \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m j \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# constant in front of exponential can be dropped because we will find peaks later\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m                 histogram_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradient_orientation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m360.\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    188\u001b[0m                 raw_histogram[histogram_index \u001b[38;5;241m%\u001b[39m num_bins] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m gradient_magnitude\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_bins):\n",
      "File \u001b[0;32m<__array_function__ internals>:2\u001b[0m, in \u001b[0;36mround_\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sfm_algo_unpacked import generateBaseImage, computeNumberOfOctaves, generateGaussianKernels, generateGaussianImages, generateDoGImages, removeDuplicateKeypoints, convertKeypointsToInputImageSize, generateDescriptors, generateDescriptors_hardware\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "img1 = cv2.imread('dino_data/dino02.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('dino_data/dino05.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "sigma=1.6 \n",
    "num_intervals=3 \n",
    "assumed_blur=0.5 \n",
    "image_border_width=5\n",
    "\n",
    "base_image = generateBaseImage(img1, sigma, assumed_blur)\n",
    "print(\"Finished base_image\")\n",
    "num_octaves = computeNumberOfOctaves(base_image.shape)\n",
    "print(\"Finished num_octaves\")\n",
    "gaussian_kernels = generateGaussianKernels(sigma, num_intervals)\n",
    "print(\"Finished gaussian_kernels\")\n",
    "gaussian_images = generateGaussianImages(base_image, num_octaves, gaussian_kernels)\n",
    "print(\"Finished gaussian_images\")\n",
    "dog_images = generateDoGImages(gaussian_images)\n",
    "print(\"Finished dog_images\")\n",
    "keypoints = findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width)\n",
    "print(\"Finished keypoints\")\n",
    "keypoints = removeDuplicateKeypoints(keypoints)\n",
    "print(\"Finished remove duplicate keypoints\")\n",
    "kp1 = convertKeypointsToInputImageSize(keypoints)\n",
    "print(\"Finished convert to image size\")\n",
    "des1 = generateDescriptors_hardware(keypoints, gaussian_images, dma)\n",
    "print(\"Finished generate_descriptors\")\n",
    "\n",
    "base_image = generateBaseImage(img2, sigma, assumed_blur)\n",
    "print(\"Finished base_image\")\n",
    "num_octaves = computeNumberOfOctaves(base_image.shape)\n",
    "print(\"Finished num_octaves\")\n",
    "gaussian_kernels = generateGaussianKernels(sigma, num_intervals)\n",
    "print(\"Finished gaussian_kernels\")\n",
    "gaussian_images = generateGaussianImages(base_image, num_octaves, gaussian_kernels)\n",
    "print(\"Finished gaussian_images\")\n",
    "dog_images = generateDoGImages(gaussian_images)\n",
    "print(\"Finished dog_images\")\n",
    "keypoints = findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width)\n",
    "print(\"Finished keypoints\")\n",
    "keypoints = removeDuplicateKeypoints(keypoints)\n",
    "print(\"Finished remove duplicate keypoints\")\n",
    "kp2 = convertKeypointsToInputImageSize(keypoints)\n",
    "print(\"Finished convert to image size\")\n",
    "des2 = generateDescriptors_hardware(keypoints, gaussian_images, dma)\n",
    "print(\"Finished generate_descriptors\")\n",
    "\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# Lowe's ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "\n",
    "# Estimate homography between template and scene\n",
    "src_pts = np.float32([ kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "M = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)[0]\n",
    "\n",
    "# Draw detected template in scene image\n",
    "h, w = img1.shape\n",
    "pts = np.float32([[0, 0],\n",
    "                    [0, h - 1],\n",
    "                    [w - 1, h - 1],\n",
    "                    [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "h1, w1 = img1.shape\n",
    "h2, w2 = img2.shape\n",
    "nWidth = w1 + w2\n",
    "nHeight = max(h1, h2)\n",
    "hdif = int((h2 - h1) / 2)\n",
    "newimg = np.zeros((nHeight, nWidth, 3), np.uint8)\n",
    "\n",
    "for i in range(3):\n",
    "    newimg[hdif:hdif + h1, :w1, i] = img1\n",
    "    newimg[:h2, w1:w1 + w2, i] = img2\n",
    "\n",
    "# Draw SIFT keypoint matches\n",
    "for m in good:\n",
    "    pt1 = (int(kp1[m.queryIdx].pt[0]), int(kp1[m.queryIdx].pt[1] + hdif))\n",
    "    pt2 = (int(kp2[m.trainIdx].pt[0] + w1), int(kp2[m.trainIdx].pt[1]))\n",
    "    cv2.line(newimg, pt1, pt2, (255, 0, 0))\n",
    "\n",
    "plt.imshow(newimg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished base_image\n",
      "Finished num_octaves\n",
      "Finished gaussian_kernels\n",
      "Finished gaussian_images\n",
      "Finished dog_images\n"
     ]
    }
   ],
   "source": [
    "from sfm_algo_unpacked import generateBaseImage, computeNumberOfOctaves, generateGaussianKernels, generateGaussianImages, generateDoGImages, removeDuplicateKeypoints, convertKeypointsToInputImageSize, generateDescriptors, generateDescriptors_hardware\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "img1 = cv2.imread('dino_data/dino02.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "sigma=1.6 \n",
    "num_intervals=3 \n",
    "assumed_blur=0.5 \n",
    "image_border_width=5\n",
    "\n",
    "base_image = generateBaseImage(img1, sigma, assumed_blur)\n",
    "print(\"Finished base_image\")\n",
    "num_octaves = computeNumberOfOctaves(base_image.shape)\n",
    "print(\"Finished num_octaves\")\n",
    "gaussian_kernels = generateGaussianKernels(sigma, num_intervals)\n",
    "print(\"Finished gaussian_kernels\")\n",
    "gaussian_images = generateGaussianImages(base_image, num_octaves, gaussian_kernels)\n",
    "print(\"Finished gaussian_images\")\n",
    "dog_images = generateDoGImages(gaussian_images)\n",
    "print(\"Finished dog_images\")\n",
    "# keypoints = findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width)\n",
    "# print(\"Finished keypoints\")\n",
    "# keypoints = removeDuplicateKeypoints(keypoints)\n",
    "# print(\"Finished remove duplicate keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a KeyPoint into a Dictionary\n",
    "def convertKPToDict(KP):\n",
    "    return {\n",
    "        'angle': KP.angle,\n",
    "        'class_id': KP.class_id,\n",
    "        'octave': KP.octave,\n",
    "        'point': KP.pt,\n",
    "        'response': KP.response,\n",
    "        'size': KP.size\n",
    "    }\n",
    "\n",
    "def convertDictToKP(D):\n",
    "    return cv2.KeyPoint(*D['point'],\n",
    "                   D['size'],\n",
    "                   D['angle'],\n",
    "                   D['response'],\n",
    "                   D['octave'])\n",
    "\n",
    "def KeyPointsToJSON(KPs):\n",
    "    output_list = []\n",
    "    for KP in KPs:\n",
    "        output_list.append(convertKPToDict(KP))\n",
    "    return output_list\n",
    "\n",
    "def unpackOctave(keypoint):\n",
    "    \"\"\"Compute octave, layer, and scale from a keypoint\n",
    "    \"\"\"\n",
    "    octave = 5308915 & 255\n",
    "    layer = (5308915 >> 8) & 255\n",
    "    if octave >= 128:\n",
    "        octave = octave | -128\n",
    "    scale = 1 / np.float32(1 << octave) if octave >= 0 else np.float32(1 << -octave)\n",
    "    return octave, layer, scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished convert to image size\n",
      "6014\n",
      "9\n",
      "243\n",
      "(-13, 1, 8192.0)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(unpackOctave(\u001b[38;5;241m2818547\u001b[39m))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(len(keypoints))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# kp1 = convertKeypointsToInputImageSize(keypoints)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m des1 \u001b[38;5;241m=\u001b[39m \u001b[43mgenerateDescriptors_hardware\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgaussian_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished generate_descriptors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/xilinx/jupyter_notebooks/Project/temp/sfm_algo_unpacked.py:296\u001b[0m, in \u001b[0;36mgenerateDescriptors_hardware\u001b[0;34m(keypoints, gaussian_images, dma, window_width, num_bins, scale_multiplier, descriptor_max_value)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keypoint \u001b[38;5;129;01min\u001b[39;00m keypoints:\n\u001b[1;32m    295\u001b[0m     octave, layer, scale \u001b[38;5;241m=\u001b[39m unpackOctave(keypoint)\n\u001b[0;32m--> 296\u001b[0m     gaussian_image \u001b[38;5;241m=\u001b[39m \u001b[43mgaussian_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43moctave\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[layer]\n\u001b[1;32m    297\u001b[0m     num_rows, num_cols \u001b[38;5;241m=\u001b[39m gaussian_image\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    298\u001b[0m     point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(scale \u001b[38;5;241m*\u001b[39m array(keypoint\u001b[38;5;241m.\u001b[39mpt))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sfm_algo_unpacked import generateBaseImage, computeNumberOfOctaves, generateGaussianKernels, generateGaussianImages, generateDoGImages, removeDuplicateKeypoints, convertKeypointsToInputImageSize, generateDescriptors, generateDescriptors_hardware\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "\n",
    "print(\"Finished convert to image size\")\n",
    "keypoints_json = []\n",
    "with open('data.json', 'r') as f:\n",
    "    pre_keypoints_json = json.load(f)\n",
    "    for kp in pre_keypoints_json:\n",
    "        keypoints_json.append(convertDictToKP(kp))\n",
    "print(len(keypoints_json))\n",
    "print(len(gaussian_images))\n",
    "print(unpackOctave(2818547))\n",
    "# print(len(keypoints))\n",
    "# kp1 = convertKeypointsToInputImageSize(keypoints)\n",
    "des1 = generateDescriptors_hardware(keypoints_json, gaussian_images, dma)\n",
    "print(\"Finished generate_descriptors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img2 = cv2.imread('dino_data/dino05.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "sigma=1.6 \n",
    "num_intervals=3 \n",
    "assumed_blur=0.5 \n",
    "image_border_width=5\n",
    "\n",
    "base_image = generateBaseImage(img1, sigma, assumed_blur)\n",
    "print(\"Finished base_image\")\n",
    "num_octaves = computeNumberOfOctaves(base_image.shape)\n",
    "print(\"Finished num_octaves\")\n",
    "gaussian_kernels = generateGaussianKernels(sigma, num_intervals)\n",
    "print(\"Finished gaussian_kernels\")\n",
    "gaussian_images = generateGaussianImages(base_image, num_octaves, gaussian_kernels)\n",
    "print(\"Finished gaussian_images\")\n",
    "dog_images = generateDoGImages(gaussian_images)\n",
    "print(\"Finished dog_images\")\n",
    "keypoints = findScaleSpaceExtrema(gaussian_images, dog_images, num_intervals, sigma, image_border_width)\n",
    "print(\"Finished keypoints\")\n",
    "keypoints = removeDuplicateKeypoints(keypoints)\n",
    "print(\"Finished remove duplicate keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kp2 = convertKeypointsToInputImageSize(keypoints)\n",
    "print(\"Finished convert to image size\")\n",
    "des2 = generateDescriptors_hardware(keypoints, gaussian_images, dma)\n",
    "print(\"Finished generate_descriptors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# Lowe's ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "\n",
    "# Estimate homography between template and scene\n",
    "src_pts = np.float32([ kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n",
    "\n",
    "M = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)[0]\n",
    "\n",
    "# Draw detected template in scene image\n",
    "h, w = img1.shape\n",
    "pts = np.float32([[0, 0],\n",
    "                    [0, h - 1],\n",
    "                    [w - 1, h - 1],\n",
    "                    [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "dst = cv2.perspectiveTransform(pts, M)\n",
    "\n",
    "img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "h1, w1 = img1.shape\n",
    "h2, w2 = img2.shape\n",
    "nWidth = w1 + w2\n",
    "nHeight = max(h1, h2)\n",
    "hdif = int((h2 - h1) / 2)\n",
    "newimg = np.zeros((nHeight, nWidth, 3), np.uint8)\n",
    "\n",
    "for i in range(3):\n",
    "    newimg[hdif:hdif + h1, :w1, i] = img1\n",
    "    newimg[:h2, w1:w1 + w2, i] = img2\n",
    "\n",
    "# Draw SIFT keypoint matches\n",
    "for m in good:\n",
    "    pt1 = (int(kp1[m.queryIdx].pt[0]), int(kp1[m.queryIdx].pt[1] + hdif))\n",
    "    pt2 = (int(kp2[m.trainIdx].pt[0] + w1), int(kp2[m.trainIdx].pt[1]))\n",
    "    cv2.line(newimg, pt1, pt2, (255, 0, 0))\n",
    "\n",
    "plt.imshow(newimg)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
